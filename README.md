# Neural Network from Scratch with NumPy and SGD
#### Overview
This project implements a simple neural network from scratch using Python, NumPy, and Stochastic Gradient Descent (SGD).
The primary goal is to build a foundational understanding of how neural networks work at the most basic level.

Features
* Custom neural network implementation using NumPy
* Training via Stochastic Gradient Descent (SGD)
* Configurable architecture (number of layers, neurons per layer)
* Activation functions: Sigmoid, ReLU, Tanh, and Softmax
* Loss functions: Mean Squared Error (MSE) and Cross-Entropy Loss
* Example datasets: MNIST and Processed Wisconsin Diagnostic Breast Cancer.
